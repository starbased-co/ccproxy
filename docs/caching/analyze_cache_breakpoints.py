#!/usr/bin/env python3
"""
Claude Code API Cache Breakpoint Analyzer

This script analyzes captured API flows to visualize cache breakpoint patterns
and generate insights about optimal cache placement strategies.
"""

import json
import glob
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any
import os
from datetime import datetime


class CacheBreakpointAnalyzer:
    def __init__(self, flows_dir: str = "captured"):
        self.flows_dir = flows_dir
        self.flows = []
        self.cache_stats = defaultdict(int)
        self.cache_locations = defaultdict(list)
        self.content_sizes = defaultdict(list)
        self.cache_patterns = []

    def load_flows(self):
        """Load all flow files from the captured directory."""
        flow_files = sorted(glob.glob(f"{self.flows_dir}/flow_*.json"))

        for flow_file in flow_files:
            with open(flow_file, "r") as f:
                flow_data = json.load(f)
                flow_num = os.path.basename(flow_file).split("_")[1].split(".")[0]
                self.flows.append({"num": flow_num, "file": flow_file, "data": flow_data})

        # Sort flows by numeric value instead of string
        self.flows.sort(key=lambda x: int(x["num"]))

        print(f"Loaded {len(self.flows)} flow files")

    def analyze_cache_breakpoints(self):
        """Analyze cache breakpoint patterns across all flows."""
        for flow in self.flows:
            data = flow["data"]
            flow_num = flow["num"]

            # Analyze system messages
            if "system" in data["request"]["body"]:
                for i, sys_msg in enumerate(data["request"]["body"]["system"]):
                    if "cache_control" in sys_msg:
                        self.cache_stats["system"] += 1
                        self.cache_locations["system"].append(
                            {"flow": flow_num, "path": f"system[{i}]", "size": len(sys_msg.get("text", ""))}
                        )
                        self.content_sizes["system"].append(len(sys_msg.get("text", "")))

            # Analyze messages
            if "messages" in data["request"]["body"]:
                for msg_idx, msg in enumerate(data["request"]["body"]["messages"]):
                    if "content" in msg and isinstance(msg["content"], list):
                        for cont_idx, content in enumerate(msg["content"]):
                            if "cache_control" in content:
                                role = msg["role"]
                                self.cache_stats[f"{role}_message"] += 1

                                # Determine content type
                                content_type = "text"
                                if content.get("type") == "tool_use":
                                    content_type = "tool_use"
                                elif content.get("type") == "tool_result":
                                    content_type = "tool_result"

                                size = len(str(content.get("text", content)))

                                self.cache_locations[f"{role}_{content_type}"].append(
                                    {
                                        "flow": flow_num,
                                        "path": f"messages[{msg_idx}].content[{cont_idx}]",
                                        "size": size,
                                        "type": content_type,
                                    }
                                )
                                self.content_sizes[f"{role}_{content_type}"].append(size)

    def generate_timeline_visualization(self, flow_num: str = "10"):
        """Generate a timeline visualization for a specific flow."""
        flow = next((f for f in self.flows if f["num"] == flow_num), None)
        if not flow:
            return "Flow not found"

        data = flow["data"]["request"]["body"]
        timeline = []
        time_ms = 0

        # System setup
        if "system" in data:
            for i, sys_msg in enumerate(data["system"]):
                cached = "üîÑ" if "cache_control" in sys_msg else "‚îÄ‚îÄ"
                size = len(sys_msg.get("text", ""))
                text_preview = sys_msg.get("text", "")[:50].replace("\n", " ")
                timeline.append(
                    {
                        "time": time_ms,
                        "type": "system",
                        "cached": cached,
                        "description": f'System: "{text_preview}..."',
                        "size": size,
                    }
                )
                time_ms += 1

        # Messages
        if "messages" in data:
            for msg_idx, msg in enumerate(data["messages"]):
                role = msg["role"]
                if "content" in msg and isinstance(msg["content"], list):
                    for cont_idx, content in enumerate(msg["content"]):
                        time_ms += 10 if role == "user" else 50

                        if "text" in content:
                            cached = "üîÑ" if "cache_control" in content else "‚îÄ‚îÄ"
                            text_preview = content["text"][:50].replace("\n", " ")
                            # Check for system reminder
                            if "<system-reminder>" in content["text"]:
                                text_preview = "System Reminder"
                            elif "Kyle's Global Assistant" in content["text"]:
                                text_preview = "Kyle's Global Assistant context"

                            timeline.append(
                                {
                                    "time": time_ms,
                                    "type": role,
                                    "cached": cached,
                                    "description": f'{role.capitalize()}: "{text_preview}..."',
                                    "size": len(content["text"]),
                                }
                            )

                        elif content.get("type") == "tool_use":
                            cached = "üîÑ" if "cache_control" in content else "‚îÄ‚îÄ"
                            tool_name = content.get("name", "Unknown")
                            tool_input = content.get("input", {})
                            preview = f"{tool_name}"
                            if "pattern" in tool_input:
                                preview += f" (pattern: {tool_input['pattern'][:20]}...)"
                            timeline.append(
                                {
                                    "time": time_ms,
                                    "type": f"{role}_tool_use",
                                    "cached": cached,
                                    "description": f"Tool Use: {preview}",
                                    "size": len(str(content)),
                                }
                            )

                        elif content.get("type") == "tool_result":
                            cached = "üîÑ" if "cache_control" in content else "‚îÄ‚îÄ"
                            result_preview = content.get("content", "")[:50].replace("\n", " ")
                            timeline.append(
                                {
                                    "time": time_ms,
                                    "type": f"{role}_tool_result",
                                    "cached": cached,
                                    "description": f'Tool Result: "{result_preview}..."',
                                    "size": len(str(content)),
                                }
                            )

        return self._format_timeline(timeline)

    def _format_timeline(self, timeline: List[Dict]) -> str:
        """Format timeline data into visual representation with bar graph."""
        output = []
        output.append("\nüìä Timeline Visualization")
        output.append("=" * 90)

        # Find max time for scaling
        max_time = max([event["time"] for event in timeline] + [100])

        # Add time axis
        output.append("\n[START] " + "‚îÄ" * 70 + "> [TIME]")

        # Create time scale markers
        scale_width = 70
        time_markers = []
        marker_positions = []

        # Generate nice round time markers
        if max_time <= 100:
            markers = [0, 25, 50, 75, 100]
        elif max_time <= 500:
            markers = [0, 100, 200, 300, 400, 500]
        elif max_time <= 1000:
            markers = [0, 200, 400, 600, 800, 1000]
        else:
            markers = [0, 500, 1000, 1500, 2000]

        # Build time scale line
        scale_line = " " * 8  # Indent to align with START
        for marker in markers:
            if marker <= max_time * 1.1:  # Only show relevant markers
                pos = int((marker / (max_time * 1.1)) * scale_width)
                time_markers.append((pos, f"{marker}ms"))

        # Build the scale with markers
        for i in range(scale_width + 1):
            found_marker = False
            for pos, label in time_markers:
                if i == pos:
                    scale_line += "‚î¨"
                    found_marker = True
                    break
            if not found_marker:
                scale_line += "‚îÄ"

        output.append(scale_line)

        # Add time labels
        label_line = " " * 8
        last_pos = -10
        for pos, label in time_markers:
            if pos - last_pos >= len(label):  # Avoid overlap
                padding = pos - len(label_line) + 8
                if padding > 0:
                    label_line += " " * padding
                label_line += label
                last_pos = pos

        output.append(label_line)
        output.append("")  # Empty line for spacing

        current_phase = None
        phase_map = {
            "system": "SYSTEM",
            "user": "USER",
            "user_tool_result": "USER",
            "assistant": "ASSISTANT",
            "assistant_tool_use": "ASSISTANT",
            "assistant_tool_result": "ASSISTANT",
        }

        # Track previous event time for duration calculation
        prev_time = 0

        for i, event in enumerate(timeline):
            # Map event type to phase
            phase = phase_map.get(event["type"], event["type"].upper())

            if phase != current_phase:
                output.append(f"‚îÇ {phase} PHASE")
                current_phase = phase

            # Calculate bar position and length
            start_pos = int((event["time"] / (max_time * 1.1)) * scale_width)

            # Estimate duration based on size and type
            if "tool" in event["type"].lower():
                duration = 20 + min(event["size"] / 100, 30)  # Tool operations
            else:
                duration = 10 + min(event["size"] / 200, 40)  # Regular messages

            # For last event or if next event is much later, use estimated duration
            if i < len(timeline) - 1:
                next_time = timeline[i + 1]["time"]
                if next_time - event["time"] < duration * 2:
                    duration = min(duration, next_time - event["time"])

            end_pos = int(((event["time"] + duration) / (max_time * 1.1)) * scale_width)

            # Build the bar
            bar = " " * (8 + start_pos)  # Indent + position

            # Add waiting time indicator
            if event["time"] > prev_time + 10:
                wait_start = int((prev_time / (max_time * 1.1)) * scale_width)
                wait_chars = start_pos - wait_start
                if wait_chars > 0:
                    bar = " " * (8 + wait_start) + "‚ñë" * wait_chars

            # Add active processing bar
            bar_length = max(1, end_pos - start_pos)
            bar += "‚ñì" * bar_length

            # Format event line with bar
            event_desc = (
                f"‚îú‚îÄ{event['cached']} [{event['time']}ms] {event['description']} ({self._format_size(event['size'])})"
            )

            # Ensure minimum spacing
            min_desc_pos = 8 + scale_width + 3
            if len(bar) < min_desc_pos:
                bar += " " * (min_desc_pos - len(bar))
            else:
                bar += "  "

            output.append(event_desc)
            output.append("‚îÇ  " + bar)

            prev_time = event["time"]

        output.append("‚îÇ")
        output.append("‚îî‚îÄ‚îÄ‚îÄ [END]\n")
        output.append("Legend: ‚ñë = waiting/idle, ‚ñì = active processing, üîÑ = cached content")

        return "\n".join(output)

    def _format_size(self, size: int) -> str:
        """Format byte size to human readable."""
        if size < 1024:
            return f"{size}B"
        elif size < 1024 * 1024:
            return f"{size / 1024:.1f}KB"
        else:
            return f"{size / (1024 * 1024):.1f}MB"

    def generate_cache_efficiency_report(self) -> str:
        """Generate a comprehensive cache efficiency report."""
        output = []
        output.append("\nüéØ Cache Efficiency Report")
        output.append("=" * 60)

        # Overall statistics
        total_caches = sum(self.cache_stats.values())
        output.append(f"\nTotal Cache Breakpoints: {total_caches}")
        output.append(f"Flows Analyzed: {len(self.flows)}")
        output.append(f"Average Caches per Flow: {total_caches / len(self.flows):.1f}")

        # Cache distribution
        output.append("\nüìä Cache Distribution:")
        for location, count in sorted(self.cache_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_caches) * 100
            bar = "‚ñà" * int(percentage / 2)
            output.append(f"  {location:20} {bar:25} {count:3} ({percentage:.1f}%)")

        # Size analysis
        output.append("\nüìè Content Size Analysis:")
        for content_type, sizes in self.content_sizes.items():
            if sizes:
                avg_size = sum(sizes) / len(sizes)
                min_size = min(sizes)
                max_size = max(sizes)
                output.append(
                    f"  {content_type:20} "
                    f"Avg: {self._format_size(int(avg_size)):8} "
                    f"Min: {self._format_size(min_size):8} "
                    f"Max: {self._format_size(max_size):8}"
                )

        # Cost analysis
        output.append("\nüí∞ Cost Impact Analysis:")
        total_cached_tokens = sum(sum(sizes) for sizes in self.content_sizes.values()) // 4

        cache_write_cost = (total_cached_tokens / 1_000_000) * 3.75
        cache_read_cost = (total_cached_tokens / 1_000_000) * 0.30
        regular_cost = (total_cached_tokens / 1_000_000) * 3.00

        output.append(f"  Total Cached Tokens: ~{total_cached_tokens:,}")
        output.append(f"  Cache Write Cost: ${cache_write_cost:.3f}")
        output.append(f"  Cache Read Cost (per hit): ${cache_read_cost:.3f}")
        output.append(f"  Regular Cost (no cache): ${regular_cost:.3f}")
        output.append(f"  Break-even after: {int(cache_write_cost / (regular_cost - cache_read_cost)) + 1} uses")

        # Recommendations
        output.append("\nüí° Optimization Recommendations:")
        output.append("  1. System messages show 100% cache usage - excellent!")
        output.append("  2. Consider caching more assistant tool results")
        output.append("  3. User message caching is selective - good for freshness")
        output.append("  4. Monitor cache hit rates in production")

        return "\n".join(output)

    def generate_pattern_analysis(self) -> str:
        """Analyze and report on caching patterns."""
        output = []
        output.append("\nüîç Cache Pattern Analysis")
        output.append("=" * 60)

        # Identify common patterns
        patterns = {
            "system_always_cached": 0,
            "tool_results_cached": 0,
            "system_reminders_cached": 0,
            "user_content_never_cached": 0,
        }

        for flow in self.flows:
            data = flow["data"]["request"]["body"]

            # Check system caching
            if "system" in data and all("cache_control" in msg for msg in data["system"]):
                patterns["system_always_cached"] += 1

            # Check message patterns
            if "messages" in data:
                for msg in data["messages"]:
                    if msg["role"] == "user" and "content" in msg:
                        has_cached = any("cache_control" in c for c in msg["content"])
                        has_uncached = any("cache_control" not in c for c in msg["content"])

                        if has_cached and has_uncached:
                            # Mixed pattern - likely system reminders are cached
                            for content in msg["content"]:
                                if "system-reminder" in content.get("text", "") and "cache_control" in content:
                                    patterns["system_reminders_cached"] += 1
                                elif "cache_control" not in content:
                                    patterns["user_content_never_cached"] += 1

                    elif msg["role"] == "assistant" and "content" in msg:
                        for content in msg["content"]:
                            if content.get("type") == "tool_use" and "cache_control" in content:
                                patterns["tool_results_cached"] += 1

        output.append("\nüìã Pattern Frequency:")
        for pattern, count in patterns.items():
            percentage = (count / len(self.flows)) * 100 if len(self.flows) > 0 else 0
            output.append(f"  {pattern.replace('_', ' ').title():40} {count:3} ({percentage:.1f}%)")

        return "\n".join(output)

    def generate_all_timelines(self) -> str:
        """Generate timeline visualizations for all flows."""
        output = []
        output.append("\nüéØ All Flow Timeline Visualizations")
        output.append("=" * 80)

        for flow in self.flows:
            flow_num = flow["num"]
            output.append(f"\n\n{'=' * 80}")
            output.append(f"FLOW {flow_num} ANALYSIS")
            output.append("=" * 80)

            timeline = self.generate_timeline_visualization(flow_num)
            output.append(timeline)

            # Add flow-specific statistics
            data = flow["data"]["request"]["body"]
            cache_count = 0
            total_size = 0

            if "system" in data:
                for msg in data["system"]:
                    if "cache_control" in msg:
                        cache_count += 1
                        total_size += len(msg.get("text", ""))

            if "messages" in data:
                for msg in data["messages"]:
                    if "content" in msg and isinstance(msg["content"], list):
                        for content in msg["content"]:
                            if "cache_control" in content:
                                cache_count += 1
                                if "text" in content:
                                    total_size += len(content["text"])
                                else:
                                    total_size += len(str(content))

            output.append(f"\nFlow {flow_num} Statistics:")
            output.append(f"‚îú‚îÄ Cache Breakpoints: {cache_count}")
            output.append(f"‚îú‚îÄ Total Cached Size: {self._format_size(total_size)}")
            output.append(f"‚îî‚îÄ Efficiency: {'High' if cache_count >= 3 else 'Medium' if cache_count >= 2 else 'Low'}")

        return "\n".join(output)

    def run_analysis(self):
        """Run the complete analysis and generate reports."""
        print("üöÄ Starting Claude Code API Cache Breakpoint Analysis")
        print("=" * 60)

        # Load flows
        self.load_flows()

        # Analyze cache breakpoints
        print("\nüìä Analyzing cache breakpoints...")
        self.analyze_cache_breakpoints()

        # Generate timeline for a sample flow
        print("\nüìà Generating timeline visualization...")
        timeline = self.generate_timeline_visualization("10")
        print(timeline)

        # Generate efficiency report
        print("\nüí∞ Generating efficiency report...")
        efficiency = self.generate_cache_efficiency_report()
        print(efficiency)

        # Generate pattern analysis
        print("\nüîç Analyzing caching patterns...")
        patterns = self.generate_pattern_analysis()
        print(patterns)

        # Generate all timelines
        print("\nüìä Generating all flow timelines...")
        all_timelines = self.generate_all_timelines()

        # Save full report
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"cache_analysis_report_{timestamp}.txt"

        with open(report_file, "w") as f:
            f.write("Claude Code API Cache Breakpoint Analysis Report\n")
            f.write("=" * 60 + "\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Flows Analyzed: {len(self.flows)}\n")
            f.write("\n" + timeline)
            f.write("\n" + efficiency)
            f.write("\n" + patterns)
            f.write("\n" + all_timelines)

        print(f"\n‚úÖ Analysis complete! Full report saved to: {report_file}")

        # Also save all timelines to a separate markdown file
        timeline_file = f"all_flow_timelines_{timestamp}.md"
        with open(timeline_file, "w") as f:
            f.write("# Claude Code API - All Flow Timeline Visualizations\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("This document contains timeline visualizations for all 31 captured API flows.\n\n")
            f.write(all_timelines.replace("üéØ All Flow Timeline Visualizations\n" + "=" * 80, ""))

        print(f"üìä All timelines saved to: {timeline_file}")


if __name__ == "__main__":
    analyzer = CacheBreakpointAnalyzer()
    analyzer.run_analysis()

