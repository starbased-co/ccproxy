# LiteLLM proxy config.yaml
model_list:
  - model_name: default # model used for `default` requests
    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input
      model: claude-3-5-sonnet-20241022 ### MODEL NAME sent to `litellm.completion()` ###
      api_base: https://api.anthropic.com
  - model_name: background # model used for `background` requests
    litellm_params:
      model: claude-3-5-haiku-20241022
      api_base: https://api.anthropic.com
  - model_name: think # model used for `think` requests
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_base: https://api.anthropic.com
  - model_name: large_context # model used for `large_context` labeled requests
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_base: https://api.anthropic.com
  - model_name: web_search # model used for `web_search` labeled requests
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_base: https://api.anthropic.com

litellm_settings:
  callbacks: custom_callbacks.ccproxy_handler

general_settings:
  monitoring:
    log_transformations: true
    metrics_enabled: true
    slow_transformation_threshold: 0.05 # 50ms in seconds

ccproxy_settings:
  context_threshold: 60000

# vim: ft=yaml
