# Task ID: 5
# Title: Build CCProxyHandler as LiteLLM CustomLogger
# Status: in-progress
# Dependencies: 3, 4
# Priority: high
# Description: Implement the main LiteLLM CustomLogger handler with async_pre_call_hook for context-aware routing and logging.
# Details:
Inherit from litellm.integrations.custom_logger.CustomLogger. In async_pre_call_hook, use RequestClassifier to label requests and ModelRouter to set the model. Log routing decisions with structured logging (use structlog or standard logging with JSON formatter). Ensure compatibility with LiteLLM v1.13+ proxy mode. Avoid logging sensitive content. Support both streaming and non-streaming requests.

# Test Strategy:
Integration test with LiteLLM proxy, verifying correct model routing and logging output for all request types.

# Subtasks:
## 1. Define CCProxyHandler Class Structure [done]
### Dependencies: None
### Description: Create the CCProxyHandler class inheriting from litellm.integrations.custom_logger.CustomLogger, ensuring all required methods for LiteLLM custom loggers are stubbed and ready for implementation.
### Details:
Set up the class skeleton with async_pre_call_hook and other relevant async logging methods. Ensure compatibility with LiteLLM v1.13+ proxy mode and prepare for structured logging integration.
<info added on 2025-07-30T19:25:41.408Z>
Implementation complete: CCProxyHandler is now fully implemented in ccproxy/handler.py, inheriting from litellm.integrations.custom_logger.CustomLogger. All required async methods—async_pre_call_hook, async_log_success_event, async_log_failure_event, and async_log_stream_event—are fully functional with structured JSON logging, request classification calls, and dynamic model routing. Code passes linting and type checks and has been verified against LiteLLM v1.13+ proxy mode. Subtask can be marked done; proceed to integrating routing logic in Subtask 5.2.
</info added on 2025-07-30T19:25:41.408Z>

## 2. Integrate Request Classification and Model Routing [done]
### Dependencies: 5.1
### Description: Implement logic in async_pre_call_hook to use RequestClassifier for labeling requests and ModelRouter to select the appropriate model based on the label.
### Details:
Call RequestClassifier.classify(request) to obtain a label, then use ModelRouter.get_model_for_label(label) to determine the model. Ensure the selected model is set in the request context for downstream processing.

## 3. Implement Structured Logging for Routing Decisions [done]
### Dependencies: 5.2
### Description: Add structured logging to record routing decisions, using structlog or standard logging with a JSON formatter, while ensuring no sensitive content is logged.
### Details:
Log key routing metadata (label, selected model, request ID, timestamp) in structured JSON format. Mask or exclude sensitive fields such as prompts, completions, or API keys.

## 4. Support Streaming and Non-Streaming Request Handling [done]
### Dependencies: 5.3
### Description: Ensure CCProxyHandler correctly handles both streaming and non-streaming requests in async_pre_call_hook and logging methods.
### Details:
Detect request type and adapt logging and routing logic as needed. Validate that all relevant events are logged for both request types without data leakage.

## 5. Validate Compatibility and Security Requirements [pending]
### Dependencies: 5.4
### Description: Test CCProxyHandler for compatibility with LiteLLM v1.13+ proxy mode and ensure no sensitive content is logged at any stage.
### Details:
Run end-to-end tests with the full proxy stack, confirming handler registration, correct operation, and strict adherence to security requirements (no logging of prompts, completions, or secrets).
<info added on 2025-07-30T19:26:31.046Z>
Initial smoke verification completed during demo:
• Ran LiteLLM in proxy mode (v1.13+) with litellm --config demo/demo_config.yaml --port 8888
• CCProxyHandler loaded from YAML, auto-registered, routed requests successfully
• Verified log output: prompts, completions, and API keys are absent or masked

Next steps – expand coverage with formal integration test suite:
1. Create pytest-based e2e tests under tests/integration/proxy/
2. Test matrix:
   – request types: chat/completion, embeddings, moderation
   – modes: streaming vs non-streaming
   – auth states: valid key, missing key, revoked key
   – routing labels: small, large, tools, fallback
   – concurrency: ≥10 parallel requests (async)
   – failure scenarios: provider 4xx/5xx, timeout, token limit
3. Assertions:
   – Correct handler registration (inspect litellm.proxy_server.custom_logger)
   – ModelRouter returns expected model per label
   – Response parity between direct and proxied calls
   – Logs contain routing metadata only; redact/mask any sensitive fields
4. Add GitHub Actions job “integration-proxy” to run the suite against a containerised LiteLLM proxy started with demo_config.yaml
5. Mark subtask complete when all tests pass and coverage ≥90 % for CCProxyHandler codepath
</info added on 2025-07-30T19:26:31.046Z>
